{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model prediction against human annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.annotation_loader import Phase1Loader, Phase1OutputLoader\n",
    "from src.utils import read_jsonlines, read_json \n",
    "import json \n",
    "from tqdm import tqdm \n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Phase1Loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_friction_indices = 0 \n",
    "\n",
    "all_conv_ids = loader.list_all_convo_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:11<00:00, 17.57it/s]\n"
     ]
    }
   ],
   "source": [
    "num_frictions = [] \n",
    "\n",
    "for elem in tqdm(all_conv_ids):\n",
    "\tfriction_turns = loader.get_annotation_data(elem)['friction_turns']\n",
    "\tnum_frictions.append(len(friction_turns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_turns = 0\n",
    "\n",
    "for elem in all_conv_ids:\n",
    "\tlength = int(elem.split('.')[0])\n",
    "\ttotal_turns += length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7950"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "np.sum(num_frictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [6], 2: [20], 3: [37]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friction_turns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model results, define overlap metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model results \n",
    "#self_c = Phase1OutputLoader(\"/fs/clip-political/rupak/common_ground/experiments/phase1_experiments/phase1_outputs/friction_prediction_self_consistency/friction_detection_temp_0.01_gpt-4o_w_gpt_assist_w_self_explanation.jsonl\")\n",
    "results_path = \"../data/model_outputs/friction_prediction_outputs\"\n",
    "\n",
    "\n",
    "gpt4o = Phase1OutputLoader(f\"{results_path}/friction_detection_temp_0.01_gpt-4o_wo_gpt_assist.jsonl\")\n",
    "gpt4omini = Phase1OutputLoader(f\"{results_path}/friction_detection_temp_0.01_gpt-4o-mini_wo_gpt_assist.jsonl\")\n",
    "gpt4omini_w_assist = Phase1OutputLoader(f\"{results_path}/friction_detection_temp_0.01_gpt-4o-mini_w_gpt_assist.jsonl\")\n",
    "gpt4o_w_assist = Phase1OutputLoader(f\"{results_path}/friction_detection_temp_0.01_gpt-4o_w_gpt_assist.jsonl\")\n",
    "llama8b = Phase1OutputLoader(f\"{results_path}/friction_detection_temp_0.01_Llama-3.1-8B-Instruct_wo_gpt_assist.jsonl\")\n",
    "llama70b = Phase1OutputLoader(f\"{results_path}/friction_detection_temp_0.01_Llama-3.1-70B-Instruct_wo_gpt_assist.jsonl\")\n",
    "llama8b_w_assist = Phase1OutputLoader(f\"{results_path}/friction_detection_temp_0.01_Llama-3.1-8B-Instruct_w_gpt_assist.jsonl\")\n",
    "llama70b_w_assist = Phase1OutputLoader(f\"{results_path}/friction_detection_temp_0.01_Llama-3.1-70B-Instruct_w_gpt_assist.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervals_intersect(interval1, interval2):\n",
    "\t\"\"\"Check if two non contiguous intervals intersect.\"\"\"\n",
    "\treturn len(set(interval1).intersection(set(interval2))) > 0\n",
    "\n",
    "def interval_jaccard(interval1, interval2):\n",
    "\t\"\"\"Computer the Jaccard similarity between two intervals.\"\"\"\n",
    "\n",
    "\tintersection = len(set(interval1).intersection(set(interval2)))\n",
    "\tunion = len(set(interval1).union(set(interval2)))\n",
    "\n",
    "\treturn intersection / union if union > 0 else 0\n",
    "\n",
    "def calculate_turn_overlap(human_response_turns, model_response_turns):\n",
    "\t# same as span overlap, but calculate turn overlap in an interval agnostic way \n",
    "\tturn_score = 0 \n",
    "\n",
    "\tmodel_response_turns = [ [i for i in range(interval[0], interval[-1] + 1)] for interval in model_response_turns]\n",
    "\n",
    "\t# flatten the two lists\n",
    "\tall_human_turns = set([turn for interval in human_response_turns for turn in interval])\n",
    "\tall_model_turns = set([turn for interval in model_response_turns for turn in interval])\n",
    "\n",
    "\t# output the turn overlap\n",
    "\tturn_score = len(all_human_turns.intersection(all_model_turns))\n",
    "\n",
    "\ttotal_human_turns = len(all_human_turns)\n",
    "\ttotal_model_turns = len(all_model_turns)\n",
    "\n",
    "\trecall = turn_score / total_human_turns if total_human_turns > 0 else 0\n",
    "\tprecision = turn_score / total_model_turns if total_model_turns > 0 else 0\n",
    "\n",
    "\tf1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "\treturn {\n",
    "\t\t\"overlap\": turn_score,\n",
    "\t\t\"total_human_intervals\": total_human_turns,\n",
    "\t\t\"total_model_intervals\": total_model_turns,\n",
    "\t\t\"precision\": precision,\n",
    "\t\t\"recall\": recall,\n",
    "\t\t\"f1\": f1\n",
    "\t}\n",
    "\n",
    "\n",
    "def calculate_span_overlap(human_response_intervals, model_response_intervals):\n",
    "\t# first, go over the human response intervals and match a unique model response interval based on the degree of overlap \n",
    "\t# if found, mark it as matched and remove it from consideration for future matches\n",
    "\tspan_score = 0 \n",
    "\n",
    "\t# fill up all the model intervals\n",
    "\tmodel_response_intervals = [ [i for i in range(interval[0], interval[-1] + 1)] for interval in model_response_intervals]\n",
    "\n",
    "\t# save a match as a tuple of indices \n",
    "\tmatches = []\n",
    "\tfor human_interval_index, human_interval in enumerate(human_response_intervals):\n",
    "\t\t# calculate the interval with the max overlap \n",
    "\t\tmax_overlap = 0\n",
    "\t\tmax_overlap_index = -1\n",
    "\t\tfor model_interval_index, model_interval in enumerate(model_response_intervals):\n",
    "\t\t\t# first check if this model interval has already been matched\n",
    "\t\t\tif model_interval_index in [m[1] for m in matches]:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\toverlap = interval_jaccard(human_interval, model_interval)\n",
    "\t\t\tif overlap > max_overlap:\n",
    "\t\t\t\tmax_overlap = overlap\n",
    "\t\t\t\tmax_overlap_index = model_interval_index\n",
    "\n",
    "\t\tif max_overlap > 0:\n",
    "\t\t\tmatches.append((human_interval_index, max_overlap_index))\n",
    "\t\t\tspan_score += max_overlap\n",
    "\n",
    "\ttotal_human_intervals = len(human_response_intervals)\n",
    "\ttotal_model_intervals = len(model_response_intervals)\n",
    "\n",
    "\t# now, recall is the span_score / total_human_intervals\n",
    "\trecall = span_score / total_human_intervals if total_human_intervals > 0 else 0\n",
    "\tprecision = span_score / total_model_intervals if total_model_intervals > 0 else 0\n",
    "\n",
    "\tf1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "\treturn {\n",
    "\t\t\"overlap\": span_score,\n",
    "\t\t\"total_human_intervals\": total_human_intervals,\n",
    "\t\t\"total_model_intervals\": total_model_intervals,\n",
    "\t\t\"precision\": precision,\n",
    "\t\t\"recall\": recall,\n",
    "\t\t\"f1\": f1\n",
    "\t}\n",
    "\n",
    "\t# Test cases for the calculate_span_overlap and calculate_overlap functions\n",
    "\n",
    "def calculate_overlap(human_response_intervals, model_response_intervals):\n",
    "\t\"\"\"\n",
    "\tCalculate precision, recall, and F1 score based on the overlap between \n",
    "\thuman response intervals and model response intervals.\n",
    "\t\n",
    "\tParameters:\n",
    "\thuman_response_intervals (list of list): List of interval(s) representing human responses.\n",
    "\tmodel_response_intervals (list of list): List of interval(s) representing model responses.\n",
    "\t\n",
    "\tReturns:\n",
    "\tdict: A dictionary containing 'overlap', 'precision', 'recall', and 'f1' scores.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\toverlap = 0\n",
    "\n",
    "\tfor model_interval in model_response_intervals:\n",
    "\t\t# fill up the model interval - \n",
    "\n",
    "\t\tmodel_interval_filled = [ i for i in range(model_interval[0], model_interval[-1] + 1)]\n",
    "\t\tfor human_interval in human_response_intervals:\n",
    "\t\t\tif intervals_intersect(model_interval_filled, human_interval):\n",
    "\t\t\t\toverlap += 1\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\ttotal_human_intervals = len(human_response_intervals)\n",
    "\ttotal_model_intervals = len(model_response_intervals)\n",
    "\t\n",
    "\trecall = overlap / total_human_intervals if total_human_intervals > 0 else 0\n",
    "\tprecision = overlap / total_model_intervals if total_model_intervals > 0 else 0\n",
    "\n",
    "\tf1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "\treturn {\n",
    "\t\t\"overlap\": overlap,\n",
    "\t\t\"total_human_intervals\": total_human_intervals,\n",
    "\t\t\"total_model_intervals\": total_model_intervals,\n",
    "\t\t\"precision\": precision,\n",
    "\t\t\"recall\": recall,\n",
    "\t\t\"f1\": f1\n",
    "\t}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small snippet showing how the overlap metrics work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span Overlap Result: {'overlap': 1.5, 'total_human_intervals': 3, 'total_model_intervals': 3, 'precision': 0.5, 'recall': 0.5, 'f1': 0.5}\n",
      "Overlap Result: {'overlap': 3, 'total_human_intervals': 3, 'total_model_intervals': 3, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n",
      "Turn Overlap Result: {'overlap': 6, 'total_human_intervals': 9, 'total_model_intervals': 9, 'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f1': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "# Define some test intervals\n",
    "human_response_intervals = [[1, 2, 3], [5, 6, 7], [10, 11, 12]]\n",
    "model_response_intervals = [[2, 4], [6, 8], [11, 13]]\n",
    "\n",
    "# Test calculate_span_overlap function\n",
    "span_overlap_result = calculate_span_overlap(human_response_intervals, model_response_intervals)\n",
    "print(\"Span Overlap Result:\", span_overlap_result)\n",
    "\n",
    "# Test calculate_overlap function\n",
    "overlap_result = calculate_overlap(human_response_intervals, model_response_intervals)\n",
    "print(\"Overlap Result:\", overlap_result)\n",
    "\n",
    "# Test calculate_turn_overlap function\n",
    "turn_overlap_result = calculate_turn_overlap(human_response_intervals, model_response_intervals)\n",
    "print(\"Turn Overlap Result:\", turn_overlap_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Some Additional test cases\n",
    "# human_response_intervals_2 = [[1, 2], [4, 8], [9, 10]]\n",
    "# model_response_intervals_2 = [[2, 3], [5, 6], [7, 8], [9, 10]]\n",
    "\n",
    "# span_overlap_result_2 = calculate_span_overlap(human_response_intervals_2, model_response_intervals_2)\n",
    "# print(\"Span Overlap Result 2:\", span_overlap_result_2)\n",
    "\n",
    "# overlap_result_2 = calculate_overlap(human_response_intervals_2, model_response_intervals_2)\n",
    "# print(\"Overlap Result 2:\", overlap_result_2)\n",
    "\n",
    "# turn_overlap_result_2 = calculate_turn_overlap(human_response_intervals_2, model_response_intervals_2)\n",
    "# print(\"Turn Overlap Result 2:\", turn_overlap_result_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Edge case: No overlap\n",
    "# human_response_intervals_3 = [[1, 2], [4, 5]]\n",
    "# model_response_intervals_3 = [[6, 7], [8, 9]]\n",
    "\n",
    "# span_overlap_result_3 = calculate_span_overlap(human_response_intervals_3, model_response_intervals_3)\n",
    "# print(\"Span Overlap Result 3:\", span_overlap_result_3)\n",
    "\n",
    "# overlap_result_3 = calculate_overlap(human_response_intervals_3, model_response_intervals_3)\n",
    "# print(\"Overlap Result 3:\", overlap_result_3)\n",
    "\n",
    "# turn_overlap_result_3 = calculate_turn_overlap(human_response_intervals_3, model_response_intervals_3)\n",
    "# print(\"Turn Overlap Result 3:\", turn_overlap_result_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [str(i) for i in range(1, 11)]\n",
    "\n",
    "all_convos = [] \n",
    "for batch in batches: \n",
    "\tloader = Phase1Loader()\n",
    "\tconvos = loader.get_convos_in_batch(batch)\n",
    "\tall_convos.extend(convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [17], 2: [38, 39]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the leader is working\n",
    "loader.get_annotation_data(\"57.1\")['friction_turns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the selected convos \n",
    "count = 0 \n",
    "# count the total number of friction turns found by humans \n",
    "\n",
    "for batch in batches:\n",
    "\tloader = Phase1Loader()\n",
    "\tconvos = loader.get_convos_in_batch(batch)\n",
    "\tfor conv in convos:\n",
    "\t\tannotation_data = loader.get_annotation_data(conv)\n",
    "\t\tfriction_turns = annotation_data['friction_turns']\n",
    "\t\tcount += len(friction_turns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to extract the spans from model outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_friction_details_corrected_new(text):\n",
    "\t# Initialize pattern to capture various parts of the input\n",
    "\tpattern = re.compile(\n",
    "\t\tr'friction_present:\\s*(true|false)\\s*|'          # Match \"friction_present: true/false\"\n",
    "\t\tr'\"friction(\\d+)\":\\s*\\[([^\\]]+)\\]|'              # Match \"frictionN\": [x, y]\n",
    "\t\tr'\"explanation(\\d+)\":\\s*\"((?:[^\"]|\\\\\")*?)\"',     # Match \"explanationN\": including any \" escaped with \\\"\n",
    "\t\tre.DOTALL\n",
    "\t)\n",
    "\n",
    "\textracted_data = {}\n",
    "\n",
    "\tfor match in pattern.finditer(text):\n",
    "\t\tif match.group(1) is not None:\n",
    "\t\t\t# Extracting the presence of friction\n",
    "\t\t\textracted_data[\"friction_present\"] = match.group(1).lower() == \"true\"\n",
    "\t\telif match.group(2) is not None:\n",
    "\t\t\t# Extracting the friction index and values\n",
    "\t\t\tindex = match.group(2)\n",
    "\t\t\t# Handle each item as a string rather than trying to convert to int immediately\n",
    "\t\t\titems = [x.strip().strip('\"') for x in match.group(3).split(\",\")]\n",
    "\t\t\textracted_data[f\"friction{index}\"] = items\n",
    "\t\telif match.group(4) is not None:\n",
    "\t\t\t# Extracting explanations\n",
    "\t\t\tindex = match.group(4)\n",
    "\t\t\texplanation = match.group(5).replace('\\\\\"', '\"')  # Correct escaped quotes\n",
    "\t\t\textracted_data[f\"explanation{index}\"] = explanation\n",
    "\n",
    "\t# Post-process to convert the friction values to integers where applicable\n",
    "\tto_remove_keys = []\n",
    "\tfor key, value in extracted_data.items():\n",
    "\t\tif re.match(r\"friction(\\d+)\", key):\n",
    "\t\t\t# Try converting items to integers or extract from strings\n",
    "\t\t\ttry:\n",
    "\t\t\t\textracted_data[key] = [int(item) for item in value]\n",
    "\t\t\texcept ValueError:\n",
    "\t\t\t\t# Handle cases where items are strings with \"Turn X\"\n",
    "\t\t\t\tif len(value[0].split()) == 1:\n",
    "\t\t\t\t\t# Discard this key and value if conversion is not applicable\n",
    "\t\t\t\t\tto_remove_keys.append(key)\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t# Extract integer from strings like \"Turn X\"\n",
    "\t\t\t\textracted_data[key] = [int(item.split(\" \")[1]) for item in value]\n",
    "\n",
    "\tfor key in to_remove_keys:\n",
    "\t\textracted_data.pop(key)\n",
    "\n",
    "\treturn extracted_data\n",
    "\n",
    "def extract_friction_details_corrected(text):\n",
    "\t# Pattern to match the JSON structure specifically for the desired keys\n",
    "\tpattern = re.compile(\n",
    "\t\tr'\"friction_present\":\\s*(true|false)|'           # Match \"friction_present\": true/false\n",
    "\t\tr'\"friction(\\d+)\":\\s*\\[([^\\]]+)\\]|'              # Match \"frictionN\": [x, y]\n",
    "\t\tr'\"explanation(\\d+)\":\\s*\"((?:[^\"]|\\\\\")*?)\"',     # Match \"explanationN\": including any \" escaped with \\\"\n",
    "\t\tre.DOTALL\n",
    "\t)\n",
    "\n",
    "\textracted_data = {}\n",
    "\n",
    "\tfor match in pattern.finditer(text):\n",
    "\t\tif match.group(1) is not None:\n",
    "\t\t\t# Extracting the presence of friction\n",
    "\t\t\textracted_data[\"friction_present\"] = match.group(1) == \"true\"\n",
    "\t\telif match.group(2) is not None:\n",
    "\t\t\t# Extracting the friction index and values\n",
    "\t\t\tindex = match.group(2)\n",
    "\t\t\t# Handle each item as a string rather than trying to convert to int\n",
    "\t\t\titems = [x.strip().strip('\"') for x in match.group(3).split(\",\")]\n",
    "\t\t\textracted_data[f\"friction{index}\"] = items\n",
    "\t\telif match.group(4) is not None:\n",
    "\t\t\t# Extracting explanations\n",
    "\t\t\tindex = match.group(4)\n",
    "\t\t\texplanation = match.group(5).replace('\\\\\"', '\"')  # Correct escaped quotes\n",
    "\t\t\textracted_data[f\"explanation{index}\"] = explanation\n",
    "\n",
    "\t# Do some post processing to convert the friction values to integers\n",
    "\t# sometimes, the model will say None, and sometimes it wil say Turn X instead of X \n",
    "\n",
    "\tto_remove_keys = []\n",
    "\tfor key, value in extracted_data.items():\n",
    "\t\t# if key is of the format \"frictionN\", then we need to convert the values to integers\n",
    "\t\tif re.match(r\"friction(\\d+)\", key):\n",
    "\t\t\t# check if all items can be converted to integers and if so, convert them\n",
    "\t\t\ttry : \n",
    "\t\t\t\textracted_data[key] = [int(item) for item in value]\n",
    "\t\t\texcept ValueError:\n",
    "\t\t\t\t# if it's a single item, then it can be converted to an integer and we can move on \n",
    "\t\t\t\tif len(value[0].split()) == 1: \n",
    "\t\t\t\t\t# discard this key and value \n",
    "\t\t\t\t\tto_remove_keys.append(key)\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t\n",
    "\t\t\t\t# if it's two items, then we need to extract the integer from the string\n",
    "\t\t\t\textracted_data[key] = [int(item.split(\" \")[1]) for item in value]\n",
    "\n",
    "\tfor key in to_remove_keys:\n",
    "\t\textracted_data.pop(key)\n",
    "\n",
    "\treturn extracted_data\n",
    "\n",
    "\n",
    "\n",
    "def get_macro_metrics(convo_ids: list[str],\n",
    "\t\t\t\t\tmodel_outputs: Phase1OutputLoader, \n",
    "\t\t\t\t\tparsing_function: callable, \n",
    "\t\t\t\t\tinterval_overlap_function: callable) -> dict: \n",
    "\t\n",
    "\toverlap = 0 \n",
    "\ttotal_human_intervals = 0\n",
    "\ttotal_model_intervals = 0\n",
    "\n",
    "\tfor convo_id in convo_ids:\n",
    "\t\t#print(convo_id)\n",
    "\n",
    "\t\t# load the human annotations \n",
    "\t\thuman_annotations = loader.get_annotation_data(convo_id)\n",
    "\t\tfriction_turns = human_annotations[\"friction_turns\"]\n",
    "\n",
    "\t\t# if no friction turns are present, skip the conversation\n",
    "\t\tif len(friction_turns) == 0:\n",
    "\t\t\t#continue\n",
    "\t\t\tpass \n",
    "\n",
    "\t\thuman_response_intervals = [interval for order, interval in friction_turns.items()]\n",
    "\n",
    "\t\t\n",
    "\t\tmodel_response = model_outputs.get_output_of_convo(convo_id)\n",
    "\t\tmodel_response_dict = parsing_function(model_response[\"response\"])\n",
    "\n",
    "\t\tif \"friction_present\" not in model_response_dict:\n",
    "\t\t\tprint(\"Something has gone wrong\")\n",
    "\n",
    "\t\t# if keys start with \"friction\", then it is a friction interval\n",
    "\t\t# make all intervals ints \n",
    "\t\tmodel_response_intervals = [interval for order, interval in model_response_dict.items() if re.match(r\"friction(\\d+)\", order)] \n",
    "\n",
    "\t\tmetrics = interval_overlap_function(human_response_intervals, model_response_intervals)\n",
    "\t\t\n",
    "\t\toverlap += metrics[\"overlap\"]\n",
    "\t\ttotal_human_intervals += metrics[\"total_human_intervals\"]\n",
    "\t\ttotal_model_intervals += metrics[\"total_model_intervals\"]\n",
    "\n",
    "\tresults =  {\n",
    "\t\t\"overlap\": overlap,\n",
    "\t\t\"total_human_intervals\": total_human_intervals,\n",
    "\t\t\"total_model_intervals\": total_model_intervals,\n",
    "\t}\n",
    "\n",
    "\tprecision = overlap / total_model_intervals if total_model_intervals > 0 else 0\n",
    "\trecall = overlap / total_human_intervals if total_human_intervals > 0 else 0\n",
    "\tf1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "\tresults[\"precision\"] = round(precision*100, 4)\n",
    "\tresults[\"recall\"] = round(recall*100, 4)\n",
    "\tresults[\"f1\"] = round(f1*100, 4)\n",
    "\n",
    "\treturn results\n",
    "\n",
    "\n",
    "def get_micro_metrics(convo_ids: list[str],\n",
    "\t\t\t\t\tmodel_outputs: Phase1OutputLoader, \n",
    "\t\t\t\t\tparsing_function: callable, \n",
    "\t\t\t\t\tinterval_overlap_function: callable) -> dict: \n",
    "\t\n",
    "\tprecision = 0\n",
    "\trecall = 0\n",
    "\tf1 = 0\n",
    "\n",
    "\tfor convo_id in convo_ids:\n",
    "\t\t#print(convo_id)\n",
    "\n",
    "\t\t# load the human annotations \n",
    "\t\thuman_annotations = loader.get_annotation_data(convo_id)\n",
    "\t\tfriction_turns = human_annotations[\"friction_turns\"]\n",
    "\n",
    "\t\t# if no friction turns are present, skip the conversation\n",
    "\t\tif len(friction_turns) == 0:\n",
    "\t\t\t#continue\n",
    "\t\t\tpass \n",
    "\n",
    "\t\thuman_response_intervals = [interval for order, interval in friction_turns.items()]\n",
    "\n",
    "\t\t\n",
    "\t\tmodel_response = model_outputs.get_output_of_convo(convo_id)\n",
    "\t\tmodel_response_dict = parsing_function(model_response[\"response\"])\n",
    "\n",
    "\t\tif \"friction_present\" not in model_response_dict:\n",
    "\t\t\tprint(\"Something has gone wrong\")\n",
    "\n",
    "\t\t# if keys start with \"friction\", then it is a friction interval\n",
    "\t\t# make all intervals ints \n",
    "\t\tmodel_response_intervals = [interval for order, interval in model_response_dict.items() if re.match(r\"friction(\\d+)\", order)] \n",
    "\n",
    "\t\tmetrics = interval_overlap_function(human_response_intervals, model_response_intervals)\n",
    "\t\t\n",
    "\t\tprecision += metrics[\"precision\"]\n",
    "\t\trecall += metrics[\"recall\"]\n",
    "\t\tf1 += metrics[\"f1\"]\n",
    "\n",
    "\tprecision = precision / len(convo_ids)\n",
    "\trecall = recall / len(convo_ids)\n",
    "\tf1 = f1 / len(convo_ids)\n",
    "\n",
    "\tresults = {}\n",
    "\tresults[\"precision\"] = round(precision*100, 4)\n",
    "\tresults[\"recall\"] = round(recall*100, 4)\n",
    "\tresults[\"f1\"] = round(f1*100, 4)\n",
    "\n",
    "\treturn results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {\n",
    "\t\"gpt4o\": gpt4o,\n",
    "\t\"gpt4o_w_explanation\": gpt4o_w_assist,\n",
    "\t\"gpt4o_mini\": gpt4omini,\n",
    "\t\"gpt4o_mini_w_explanation\": gpt4omini_w_assist,\n",
    "\t\"llama_8b\": llama8b,\n",
    "\t\"llama_8b_w_explanation\": llama8b_w_assist, \n",
    "\t\"llama_70b\": llama70b,\n",
    "\t\"llama_70b_w_explanation\": llama70b_w_assist\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Function: get_micro_metrics\n",
      "Interval Matching Function: calculate_overlap\n",
      "Model: gpt4o: Precision: 31.5, Recall: 43.6937, F1: 34.011\n",
      "Model: gpt4o_w_explanation: Precision: 31.6333, Recall: 37.4568, F1: 32.2195\n",
      "Model: gpt4o_mini: Precision: 32.75, Recall: 27.8585, F1: 28.0058\n",
      "Model: gpt4o_mini_w_explanation: Precision: 28.5411, Recall: 28.6689, F1: 26.5134\n",
      "Model: llama_8b: Precision: 16.7171, Recall: 47.2779, F1: 22.531\n",
      "Model: llama_8b_w_explanation: Precision: 15.977, Recall: 46.3289, F1: 21.7343\n",
      "Model: llama_70b: Precision: 21.6966, Recall: 48.0949, F1: 27.9698\n",
      "Model: llama_70b_w_explanation: Precision: 16.7179, Recall: 39.8273, F1: 22.0649\n",
      "\n",
      "\n",
      "Interval Matching Function: calculate_span_overlap\n",
      "Model: gpt4o: Precision: 13.4995, Recall: 18.7378, F1: 14.6067\n",
      "Model: gpt4o_w_explanation: Precision: 13.5358, Recall: 16.5915, F1: 14.003\n",
      "Model: gpt4o_mini: Precision: 13.6653, Recall: 12.3207, F1: 12.1031\n",
      "Model: gpt4o_mini_w_explanation: Precision: 13.6279, Recall: 14.1069, F1: 12.8093\n",
      "Model: llama_8b: Precision: 6.8691, Recall: 18.7193, F1: 9.1402\n",
      "Model: llama_8b_w_explanation: Precision: 7.1098, Recall: 20.0207, F1: 9.5764\n",
      "Model: llama_70b: Precision: 8.9282, Recall: 20.2634, F1: 11.5863\n",
      "Model: llama_70b_w_explanation: Precision: 7.3486, Recall: 16.7564, F1: 9.5202\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metric Function: get_macro_metrics\n",
      "Interval Matching Function: calculate_overlap\n",
      "Model: gpt4o: Precision: 34.3434, Recall: 71.4286, F1: 46.3847\n",
      "Model: gpt4o_w_explanation: Precision: 34.9425, Recall: 63.8655, F1: 45.1709\n",
      "Model: gpt4o_mini: Precision: 32.2785, Recall: 42.8571, F1: 36.8231\n",
      "Model: gpt4o_mini_w_explanation: Precision: 28.3163, Recall: 46.6387, F1: 35.2381\n",
      "Model: llama_8b: Precision: 14.8206, Recall: 79.8319, F1: 25.0\n",
      "Model: llama_8b_w_explanation: Precision: 15.8819, Recall: 83.6134, F1: 26.6935\n",
      "Model: llama_70b: Precision: 22.4037, Recall: 80.6723, F1: 35.0685\n",
      "Model: llama_70b_w_explanation: Precision: 16.8926, Recall: 68.0672, F1: 27.0677\n",
      "\n",
      "\n",
      "Interval Matching Function: calculate_span_overlap\n",
      "Model: gpt4o: Precision: 14.2816, Recall: 29.7033, F1: 19.2889\n",
      "Model: gpt4o_w_explanation: Precision: 14.7521, Recall: 26.9629, F1: 19.0704\n",
      "Model: gpt4o_mini: Precision: 13.9698, Recall: 18.5482, F1: 15.9367\n",
      "Model: gpt4o_mini_w_explanation: Precision: 14.105, Recall: 23.2317, F1: 17.5528\n",
      "Model: llama_8b: Precision: 5.8814, Recall: 31.6803, F1: 9.9209\n",
      "Model: llama_8b_w_explanation: Precision: 6.3683, Recall: 33.527, F1: 10.7035\n",
      "Model: llama_70b: Precision: 9.0608, Recall: 32.6266, F1: 14.1829\n",
      "Model: llama_70b_w_explanation: Precision: 7.083, Recall: 28.5405, F1: 11.3494\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for metric_function in [get_micro_metrics, get_macro_metrics]:\t\n",
    "\tprint(f\"Metric Function: {metric_function.__name__}\")\n",
    "\tfor interval_matching_function in [calculate_overlap,  calculate_span_overlap]: \n",
    "\t\tprint(f\"Interval Matching Function: {interval_matching_function.__name__}\")\n",
    "\t\tfor model_name, model_output in model_map.items():\n",
    "\t\t\t\n",
    "\t\t\tresults = metric_function(all_convos, model_output, extract_friction_details_corrected, interval_matching_function)\n",
    "\t\t\tprecision, recall, f1 = results[\"precision\"], results[\"recall\"], results[\"f1\"]\n",
    "\t\t\tprint(f\"Model: {model_name}: Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "\t\t\n",
    "\t\tprint(\"\\n\")\n",
    "\tprint(\"\\n\")\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite the macro and micro averaging code to work with DistilRoberta outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macro_metrics_dr(convo_ids: list[str],\n",
    "\t\t\t\t\tmodel_outputs: dict,\n",
    "\t\t\t\t\tinterval_overlap_function: callable) -> dict: \n",
    "\t\n",
    "\toverlap = 0 \n",
    "\ttotal_human_intervals = 0\n",
    "\ttotal_model_intervals = 0\n",
    "\n",
    "\tfor convo_id in convo_ids:\n",
    "\t\t#print(convo_id)\n",
    "\n",
    "\t\t# load the human annotations \n",
    "\t\thuman_annotations = loader.get_annotation_data(convo_id)\n",
    "\t\tmodel_response_intervals = model_outputs[convo_id]\n",
    "\t\tfriction_turns = human_annotations[\"friction_turns\"]\n",
    "\n",
    "\t\t# if no friction turns are present, skip the conversation\n",
    "\t\tif len(friction_turns) == 0:\n",
    "\t\t\t#continue\n",
    "\t\t\tpass \n",
    "\n",
    "\t\thuman_response_intervals = [interval for order, interval in friction_turns.items()]\n",
    "\t\t# model_response = model_outputs.get_output_of_convo(convo_id)\n",
    "\t\t# model_response_dict = parsing_function(model_response[\"response\"])\n",
    "\n",
    "\t\t# if \"friction_present\" not in model_response_dict:\n",
    "\t\t# \tprint(\"Something has gone wrong\")\n",
    "\n",
    "\t\t# # if keys start with \"friction\", then it is a friction interval\n",
    "\t\t# # make all intervals ints \n",
    "\t\t# model_response_intervals = [interval for order, interval in model_response_dict.items() if re.match(r\"friction(\\d+)\", order)] \n",
    "\n",
    "\t\tmetrics = interval_overlap_function(human_response_intervals, model_response_intervals)\n",
    "\t\t\n",
    "\t\toverlap += metrics[\"overlap\"]\n",
    "\t\ttotal_human_intervals += metrics[\"total_human_intervals\"]\n",
    "\t\ttotal_model_intervals += metrics[\"total_model_intervals\"]\n",
    "\n",
    "\tresults =  {\n",
    "\t\t\"overlap\": overlap,\n",
    "\t\t\"total_human_intervals\": total_human_intervals,\n",
    "\t\t\"total_model_intervals\": total_model_intervals,\n",
    "\t}\n",
    "\n",
    "\tprecision = overlap / total_model_intervals if total_model_intervals > 0 else 0\n",
    "\trecall = overlap / total_human_intervals if total_human_intervals > 0 else 0\n",
    "\tf1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "\tresults[\"precision\"] = round(precision*100, 4)\n",
    "\tresults[\"recall\"] = round(recall*100, 4)\n",
    "\tresults[\"f1\"] = round(f1*100, 4)\n",
    "\n",
    "\treturn results\n",
    "\n",
    "\n",
    "def get_micro_metrics_dr(convo_ids: list[str],\n",
    "\t\t\t\t\tmodel_outputs: dict,\n",
    "\t\t\t\t\tinterval_overlap_function: callable) -> dict: \n",
    "\t\n",
    "\tprecision = 0\n",
    "\trecall = 0\n",
    "\tf1 = 0\n",
    "\n",
    "\tfor convo_id in convo_ids:\n",
    "\t\t#print(convo_id)\n",
    "\n",
    "\t\t# load the human annotations \n",
    "\t\thuman_annotations = loader.get_annotation_data(convo_id)\n",
    "\t\tmodel_response_intervals = model_outputs[convo_id]\n",
    "\t\tfriction_turns = human_annotations[\"friction_turns\"]\n",
    "\n",
    "\t\t# if no friction turns are present, skip the conversation\n",
    "\t\tif len(friction_turns) == 0:\n",
    "\t\t\t#continue\n",
    "\t\t\tpass \n",
    "\n",
    "\t\thuman_response_intervals = [interval for order, interval in friction_turns.items()]\n",
    "\n",
    "\t\tmetrics = interval_overlap_function(human_response_intervals, model_response_intervals)\n",
    "\t\t\n",
    "\t\tprecision += metrics[\"precision\"]\n",
    "\t\trecall += metrics[\"recall\"]\n",
    "\t\tf1 += metrics[\"f1\"]\n",
    "\n",
    "\tprecision = precision / len(convo_ids)\n",
    "\trecall = recall / len(convo_ids)\n",
    "\tf1 = f1 / len(convo_ids)\n",
    "\n",
    "\tresults = {}\n",
    "\tresults[\"precision\"] = round(precision*100, 4)\n",
    "\tresults[\"recall\"] = round(recall*100, 4)\n",
    "\tresults[\"f1\"] = round(f1*100, 4)\n",
    "\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, calculate results for distilroberta with context 3 and context 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of distilroberta, we are doing k-fold cross validation, so we need to load the batch to test map\n",
    "batch_test_map = read_json(\"../data/model_outputs/distilroberta_outputs/batch_to_test_map.json\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def find_intervals_by_conv_id(conv_id, df):\n",
    "\t# Load the CSV file into a DataFrame\n",
    "\t# first, turn all conv_id to strings\n",
    "\tdf['conv_id'] = df['conv_id'].astype(str)\n",
    "\n",
    "\t# Filter rows by the given conv_id\n",
    "\tfiltered_df = df[df['conv_id'] == conv_id]\n",
    "\n",
    "\t# if that conv_id is not found, raise an error \n",
    "\tif filtered_df.empty:\n",
    "\t\t# try to see if the float() of the conv_id is in the dataframe\n",
    "\t\ttry:\n",
    "\t\t\tfiltered_df = df[df['conv_id'] == str(int(float(conv_id)))]\n",
    "\t\texcept:\n",
    "\t\t\traise ValueError(f\"Conversation ID {conv_id} not found in the DataFrame.\")\n",
    "\t\n",
    "\t# Sort the filtered DataFrame by turn_id\n",
    "\tsorted_df = filtered_df.sort_values(by='turn_id')\n",
    "\t\n",
    "\t# Initialize a list to store the intervals\n",
    "\tintervals = []\n",
    "\t\n",
    "\t# Initialize a list to collect turn_ids for the current interval\n",
    "\tcurrent_interval = []\n",
    "\n",
    "\t# Iterate over each row in the sorted DataFrame\n",
    "\tfor _, row in sorted_df.iterrows():\n",
    "\t\tturn_id = row['turn_id']\n",
    "\t\tlabel = row['prediction']\n",
    "\n",
    "\t\tif label == 1:\n",
    "\t\t\tcurrent_interval.append(turn_id)  # Add to current interval\n",
    "\t\telse:\n",
    "\t\t\tif current_interval:\n",
    "\t\t\t\tintervals.append(current_interval)  # Save the interval\n",
    "\t\t\t\tcurrent_interval = []  # Clear for next interval\n",
    "\n",
    "\t# Check if there's an unfinished interval\n",
    "\tif current_interval:\n",
    "\t\tintervals.append(current_interval)\n",
    "\n",
    "\treturn intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_metrics(context, batch_test_map, metric_function, model_name=\"distilroberta\"):\n",
    "\t\"\"\"\n",
    "\tInput:\n",
    "\tcontext: 3 or 5 \n",
    "\tbatch_test_map: dict mapping batch index to list of conversation ids\n",
    "\tmetric_function: micro or macro averaging \n",
    "\t\n",
    "\tOutput: \n",
    "\t\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport numpy as np\n",
    "\tfrom src.utils import read_jsonlines\n",
    "\t\n",
    "\t# Initialize metric lists\n",
    "\toverlap_precisions = []\n",
    "\toverlap_recalls = []\n",
    "\toverlap_f1s = []\n",
    "\t\n",
    "\tspan_overlap_precisions = []\n",
    "\tspan_overlap_recalls = [] \n",
    "\tspan_overlap_f1s = []\n",
    "\t\n",
    "\tprint(f\"Metric Function: {metric_function.__name__}\")\n",
    "\t\t\n",
    "\tfor index, convo_id_list in tqdm(enumerate(batch_test_map.values())):\n",
    "\t\tpath_to_data_file = f\"../data/model_outputs/{model_name}_outputs/context_{context}/split_{index}/test.csv\"\n",
    "\t\tpath_to_predictions = f\"../data/model_outputs/{model_name}_outputs/context_{context}/split_{index}/predict_results_None.txt\"\n",
    "\t\t\n",
    "\t\tdf = pd.read_csv(path_to_data_file)\n",
    "\t\tpredictions = read_jsonlines(path_to_predictions)\n",
    "\t\tpreds = [p[\"prediction\"] for p in predictions]\n",
    "\t\tdf[\"prediction\"] = preds\n",
    "\n",
    "\t\tmodel_output_dict = {}\n",
    "\n",
    "\t\tfor conv_id in convo_id_list: \n",
    "\t\t\tintervals = find_intervals_by_conv_id(conv_id, df)\n",
    "\t\t\tmodel_output_dict[conv_id] = intervals\n",
    "\n",
    "\t\tfor interval_matching_function in [calculate_overlap, calculate_span_overlap]:\n",
    "\t\t\t#print(f\"Interval Matching Function: {interval_matching_function.__name__}\")\n",
    "\t\t\tresults = metric_function(convo_id_list, model_output_dict, interval_matching_function)\n",
    "\t\t\tprecision, recall, f1 = results[\"precision\"], results[\"recall\"], results[\"f1\"]\n",
    "\t\t\t#print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "\n",
    "\t\t\tif interval_matching_function == calculate_overlap:\n",
    "\t\t\t\toverlap_precisions.append(precision)\n",
    "\t\t\t\toverlap_recalls.append(recall)\n",
    "\t\t\t\toverlap_f1s.append(f1)\n",
    "\n",
    "\t\t\telif interval_matching_function == calculate_span_overlap:\n",
    "\t\t\t\tspan_overlap_precisions.append(precision)\n",
    "\t\t\t\tspan_overlap_recalls.append(recall)\n",
    "\t\t\t\tspan_overlap_f1s.append(f1)\n",
    "\t\n",
    "\t\t\t\t#print(\"\\n\")\n",
    "\t\t\t#print(\"\\n\")\n",
    "\t\n",
    "\t# Compute means\n",
    "\tresults = {\n",
    "\t\t\"overlap_precision_mean\": np.mean(overlap_precisions),\n",
    "\t\t\"overlap_recall_mean\": np.mean(overlap_recalls),\n",
    "\t\t\"overlap_f1_mean\": np.mean(overlap_f1s),\n",
    "\t\t\"span_overlap_precision_mean\": np.mean(span_overlap_precisions),\n",
    "\t\t\"span_overlap_recall_mean\": np.mean(span_overlap_recalls),\n",
    "\t\t\"span_overlap_f1_mean\": np.mean(span_overlap_f1s)\n",
    "\t}\n",
    "\t\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Function: get_macro_metrics_dr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:31,  6.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Function: get_macro_metrics_dr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:31,  6.32s/it]\n"
     ]
    }
   ],
   "source": [
    "# get macro-averaged results for distilroberta with context 3 and 5\n",
    "\n",
    "macro_3 = compute_model_metrics(3, batch_test_map, get_macro_metrics_dr, model_name=\"distilroberta\")\n",
    "macro_5 = compute_model_metrics(5, batch_test_map, get_macro_metrics_dr, model_name=\"distilroberta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Function: get_micro_metrics_dr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:31,  6.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Function: get_micro_metrics_dr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:31,  6.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# compute micro-averaged results for distilroberta with context 3 and 5\n",
    "\n",
    "micro_3 = compute_model_metrics(3, batch_test_map, get_micro_metrics_dr, model_name=\"distilroberta\")\n",
    "micro_5 = compute_model_metrics(5, batch_test_map, get_micro_metrics_dr, model_name=\"distilroberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overlap_precision_mean': 14.5155,\n",
       " 'overlap_recall_mean': 26.32228,\n",
       " 'overlap_f1_mean': 16.36644,\n",
       " 'span_overlap_precision_mean': 5.93104,\n",
       " 'span_overlap_recall_mean': 10.12008,\n",
       " 'span_overlap_f1_mean': 6.56902}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "micro_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overlap_precision_mean': 13.0267,\n",
       " 'overlap_recall_mean': 26.13882,\n",
       " 'overlap_f1_mean': 15.5173,\n",
       " 'span_overlap_precision_mean': 6.3838,\n",
       " 'span_overlap_recall_mean': 11.84358,\n",
       " 'span_overlap_f1_mean': 7.431660000000001}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "micro_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conv-cg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
